{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing University of Alabama...\n",
      "Processing University of Arkansas...\n",
      "Processing Auburn University...\n",
      "Processing University of Florida...\n",
      "Processing University of Georgia...\n",
      "Processing University of Kentucky...\n",
      "Processing Louisiana State University...\n",
      "Processing University of Mississippi...\n",
      "Processing Mississippi State University...\n",
      "Processing University of Missouri...\n",
      "Processing University of Oklahoma...\n",
      "Processing University of South Carolina...\n",
      "Processing University of Tennessee...\n",
      "Processing University of Texas at Austin...\n",
      "Processing Texas A&M University...\n",
      "Processing Vanderbilt University...\n",
      "Finished processing 16 universities in 27.33 seconds\n"
     ]
    }
   ],
   "source": [
    "# Import the requests module\n",
    "import requests\n",
    "# We don't need everything from bs4, just BeautifulSoup\n",
    "from bs4 import BeautifulSoup\n",
    "# Import pandas and assign the standard shorthand name `pd` to it\n",
    "import pandas as pd\n",
    "# Import the standard module for regular expressions (`re`)\n",
    "import re\n",
    "# Import the standard module `time`\n",
    "import time\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# URL of the SEC Wikipedia page\n",
    "url = 'https://en.wikipedia.org/wiki/Southeastern_Conference'\n",
    "\n",
    "# Use `requests` to 'get' the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if request was successful\n",
    "if response.status_code == 200:\n",
    "    # Make the \"soup\"\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    # Find the heading with id=\"Members\"\n",
    "    members_heading = soup.find('h3', id='Members')\n",
    "\n",
    "    # Find the next table element after this heading\n",
    "    target_table = members_heading.find_next('table')\n",
    "    \n",
    "    # Initialize a Python list to store university data\n",
    "    universities = []\n",
    "            \n",
    "    # Skip the header row and process each row in the table body\n",
    "    rows = target_table.find_all('tr')[1:]  # Skip the header row\n",
    "            \n",
    "    for row in rows:\n",
    "        cells = row.find_all('th')\n",
    "        if cells:\n",
    "            # Look for the university name cell with a link\n",
    "            univ_cell = cells[0]  # First column contains university name\n",
    "            link = univ_cell.find('a')\n",
    "            \n",
    "            if link:\n",
    "                # Use the `get_text` method to extract the text between the tags\n",
    "                univ_name = link.get_text(strip=True)\n",
    "                # Use the `get` method to extract the value of the `href` attribute\n",
    "                relative_url = link.get('href')\n",
    "                # Combine the base URL with the relative URL\n",
    "                univ_url = \"https://en.wikipedia.org\" + relative_url\n",
    "                # Make a dictionary of the university name and URL and add it to the list\n",
    "                universities.append({\n",
    "                    'University': univ_name,\n",
    "                    'Wikipedia URL': univ_url\n",
    "                })\n",
    "\n",
    "# Create a dataframe with the universities and their URLs\n",
    "df = pd.DataFrame(universities)\n",
    "    \n",
    "# Make a Python dictionary to store individual university dataframes\n",
    "university_dataframes = {}\n",
    "    \n",
    "# Function to clean up text in infobox tables\n",
    "def clean_text(text):\n",
    "    # Use regular expressions to remove citations like \"[1]\" and \"[2]\"\n",
    "    text = re.sub(r'\\[\\d+\\]', '', text)\n",
    "    # Use regular expressions to remove line breaks and extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "    \n",
    "# Request each university's page and extract its infobox table\n",
    "for index, row in df.iterrows():\n",
    "    univ_name = row['University']\n",
    "    univ_url = row['Wikipedia URL']\n",
    "    \n",
    "    # Create a simplified name for the dataframe variable\n",
    "    simple_name = re.sub(r'[^a-zA-Z0-9]', '_', univ_name).lower()\n",
    "    \n",
    "    print(f\"Processing {univ_name}...\")\n",
    "    \n",
    "    # Send a GET request to the university's Wikipedia page\n",
    "    univ_response = requests.get(univ_url)\n",
    "    \n",
    "    # Add a short delay to play nicely with Wikipedia's servers\n",
    "    time.sleep(1)\n",
    "    \n",
    "    if univ_response.status_code == 200:\n",
    "        # Make the univ_soup\n",
    "        univ_soup = BeautifulSoup(univ_response.content, 'html.parser')\n",
    "        \n",
    "        # Find the infobox table\n",
    "        infobox = univ_soup.find('table', class_='infobox vcard')\n",
    "        \n",
    "        if infobox:\n",
    "            # Initialize a list to hold the rows from the infobox\n",
    "            infobox_data = []\n",
    "            \n",
    "            # Process each row in the infobox\n",
    "            for tr in infobox.find_all('tr'):\n",
    "                # Check if row has th (header/label) and td (data)\n",
    "                th = tr.find('th')\n",
    "                td = tr.find('td')\n",
    "                \n",
    "                if th and td:\n",
    "                    # Clean the text with the function we made above\n",
    "                    label = clean_text(th.get_text())\n",
    "                    value = clean_text(td.get_text())\n",
    "                    \n",
    "                    # Append the label and value as a dictionary to the infobox_data list\n",
    "                    infobox_data.append({\n",
    "                        'Property': label,\n",
    "                        'Value': value\n",
    "                    })\n",
    "            \n",
    "            # Create a dataframe for this university\n",
    "            if infobox_data:\n",
    "                univ_df = pd.DataFrame(infobox_data)\n",
    "                university_dataframes[simple_name] = univ_df\n",
    "\n",
    "# Get a list of all properties for each university\n",
    "university_properties = {}\n",
    "for univ_name, univ_df in university_dataframes.items():\n",
    "    university_properties[univ_name] = set(univ_df['Property'].tolist())\n",
    "\n",
    "# Find the intersection of all sets of properties\n",
    "all_universities = list(university_properties.keys())\n",
    "if all_universities:\n",
    "    common_properties = university_properties[all_universities[0]].copy()\n",
    "    # Use the `intersection` method to find the common properties\n",
    "    for univ_name in all_universities[1:]:\n",
    "        common_properties = common_properties.intersection(university_properties[univ_name])\n",
    "    \n",
    "    # Sort the list and display it\n",
    "    common_properties_list = sorted(list(common_properties))\n",
    "\n",
    "# Create a new dataframe with universities as rows and common properties as columns\n",
    "consolidated_data = []\n",
    "\n",
    "for univ_name, univ_df in university_dataframes.items():\n",
    "    # Start with the university name\n",
    "    univ_data = {'University': univ_name}\n",
    "    \n",
    "    # Add each common property value\n",
    "    for prop in common_properties_list:\n",
    "        property_row = univ_df[univ_df['Property'] == prop]\n",
    "        if not property_row.empty:\n",
    "            univ_data[prop] = property_row['Value'].iloc[0]\n",
    "        else:\n",
    "            univ_data[prop] = None\n",
    "    \n",
    "    consolidated_data.append(univ_data)\n",
    "\n",
    "# Create the consolidated dataframe\n",
    "consolidated_df = pd.DataFrame(consolidated_data)\n",
    "\n",
    "# Clean up university names for display (remove underscores, capitalize words)\n",
    "consolidated_df['University'] = consolidated_df['University'].apply(\n",
    "    lambda x: ' '.join(word.capitalize() for word in x.replace('_', ' ').split())\n",
    ")\n",
    "\n",
    "# Save the DataFrame to a CSV file (optional)\n",
    "consolidated_df.to_excel('sec_universities.xlsx', index=False)\n",
    "\n",
    "# Calculate elapsed time\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Finished processing {len(consolidated_df)} universities in {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webscraping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
