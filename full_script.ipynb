{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'universities' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 58\u001b[0m\n\u001b[1;32m     52\u001b[0m                 universities\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     53\u001b[0m                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUniversity\u001b[39m\u001b[38;5;124m'\u001b[39m: univ_name,\n\u001b[1;32m     54\u001b[0m                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWikipedia URL\u001b[39m\u001b[38;5;124m'\u001b[39m: univ_url\n\u001b[1;32m     55\u001b[0m                 })\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Create a dataframe with the universities and their URLs\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43muniversities\u001b[49m)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Make a Python dictionary to store individual university dataframes\u001b[39;00m\n\u001b[1;32m     61\u001b[0m university_dataframes \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'universities' is not defined"
     ]
    }
   ],
   "source": [
    "# Import the requests module\n",
    "import requests\n",
    "# We don't need everything from bs4, just BeautifulSoup\n",
    "from bs4 import BeautifulSoup\n",
    "# Import pandas and assign the standard shorthand name `pd` to it\n",
    "import pandas as pd\n",
    "# Import the standard module for regular expressions (`re`)\n",
    "import re\n",
    "# Import the standard module `time`\n",
    "import time\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# URL of the SEC Wikipedia page\n",
    "url = 'https://en.wikipedia.org/wiki/Southeastern_Conference'\n",
    "\n",
    "# Use `requests` to 'get' the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if request was successful\n",
    "if response.status_code == 200:\n",
    "    # Make the \"soup\"\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    # Find the heading with id=\"Members\"\n",
    "    members_heading = soup.find('h3', id='Members')\n",
    "\n",
    "    # Find the next table element after this heading\n",
    "    target_table = members_heading.find_next('table')\n",
    "    \n",
    "    # Initialize a Python list to store university data\n",
    "    universities = []\n",
    "            \n",
    "    # Skip the header row and process each row in the table body\n",
    "    rows = target_table.find_all('tr')[1:]  # Skip the header row\n",
    "            \n",
    "    for row in rows:\n",
    "        cells = row.find_all('th')\n",
    "        if cells:\n",
    "            # Look for the university name cell with a link\n",
    "            univ_cell = cells[0]  # First column contains university name\n",
    "            link = univ_cell.find('a')\n",
    "            \n",
    "            if link:\n",
    "                # Use the `get_text` method to extract the text between the tags\n",
    "                univ_name = link.get_text(strip=True)\n",
    "                # Use the `get` method to extract the value of the `href` attribute\n",
    "                relative_url = link.get('href')\n",
    "                # Combine the base URL with the relative URL\n",
    "                univ_url = \"https://en.wikipedia.org\" + relative_url\n",
    "                # Make a dictionary of the university name and URL and add it to the list\n",
    "                universities.append({\n",
    "                    'University': univ_name,\n",
    "                    'Wikipedia URL': univ_url\n",
    "                })\n",
    "\n",
    "# Create a dataframe with the universities and their URLs\n",
    "df = pd.DataFrame(universities)\n",
    "    \n",
    "# Make a Python dictionary to store individual university dataframes\n",
    "university_dataframes = {}\n",
    "    \n",
    "# Function to clean up text in infobox tables\n",
    "def clean_text(text):\n",
    "    # Use regular expressions to remove citations like \"[1]\" and \"[2]\"\n",
    "    text = re.sub(r'\\[\\d+\\]', '', text)\n",
    "    # Use regular expressions to remove line breaks and extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "    \n",
    "# Request each university's page and extract its infobox table\n",
    "for index, row in df.iterrows():\n",
    "    univ_name = row['University']\n",
    "    univ_url = row['Wikipedia URL']\n",
    "    \n",
    "    # Create a simplified name for the dataframe variable\n",
    "    simple_name = re.sub(r'[^a-zA-Z0-9]', '_', univ_name).lower()\n",
    "    \n",
    "    print(f\"Processing {univ_name}...\")\n",
    "    \n",
    "    # Send a GET request to the university's Wikipedia page\n",
    "    univ_response = requests.get(univ_url)\n",
    "    \n",
    "    # Add a short delay to play nicely with Wikipedia's servers\n",
    "    time.sleep(1)\n",
    "    \n",
    "    if univ_response.status_code == 200:\n",
    "        # Make the univ_soup\n",
    "        univ_soup = BeautifulSoup(univ_response.content, 'html.parser')\n",
    "        \n",
    "        # Find the infobox table\n",
    "        infobox = univ_soup.find('table', class_='infobox vcard')\n",
    "        \n",
    "        if infobox:\n",
    "            # Initialize a list to hold the rows from the infobox\n",
    "            infobox_data = []\n",
    "            \n",
    "            # Process each row in the infobox\n",
    "            for tr in infobox.find_all('tr'):\n",
    "                # Check if row has th (header/label) and td (data)\n",
    "                th = tr.find('th')\n",
    "                td = tr.find('td')\n",
    "                \n",
    "                if th and td:\n",
    "                    # Clean the text with the function we made above\n",
    "                    label = clean_text(th.get_text())\n",
    "                    value = clean_text(td.get_text())\n",
    "                    \n",
    "                    # Append the label and value as a dictionary to the infobox_data list\n",
    "                    infobox_data.append({\n",
    "                        'Property': label,\n",
    "                        'Value': value\n",
    "                    })\n",
    "            \n",
    "            # Create a dataframe for this university\n",
    "            if infobox_data:\n",
    "                univ_df = pd.DataFrame(infobox_data)\n",
    "                university_dataframes[simple_name] = univ_df\n",
    "\n",
    "# Get a list of all properties for each university\n",
    "university_properties = {}\n",
    "for univ_name, univ_df in university_dataframes.items():\n",
    "    university_properties[univ_name] = set(univ_df['Property'].tolist())\n",
    "\n",
    "# Find the intersection of all sets of properties\n",
    "all_universities = list(university_properties.keys())\n",
    "if all_universities:\n",
    "    common_properties = university_properties[all_universities[0]].copy()\n",
    "    # Use the `intersection` method to find the common properties\n",
    "    for univ_name in all_universities[1:]:\n",
    "        common_properties = common_properties.intersection(university_properties[univ_name])\n",
    "    \n",
    "    # Sort the list and display it\n",
    "    common_properties_list = sorted(list(common_properties))\n",
    "\n",
    "# Create a new dataframe with universities as rows and common properties as columns\n",
    "consolidated_data = []\n",
    "\n",
    "for univ_name, univ_df in university_dataframes.items():\n",
    "    # Start with the university name\n",
    "    univ_data = {'University': univ_name}\n",
    "    \n",
    "    # Add each common property value\n",
    "    for prop in common_properties_list:\n",
    "        property_row = univ_df[univ_df['Property'] == prop]\n",
    "        if not property_row.empty:\n",
    "            univ_data[prop] = property_row['Value'].iloc[0]\n",
    "        else:\n",
    "            univ_data[prop] = None\n",
    "    \n",
    "    consolidated_data.append(univ_data)\n",
    "\n",
    "# Create the consolidated dataframe\n",
    "consolidated_df = pd.DataFrame(consolidated_data)\n",
    "\n",
    "# Clean up university names for display (remove underscores, capitalize words)\n",
    "consolidated_df['University'] = consolidated_df['University'].apply(\n",
    "    lambda x: ' '.join(word.capitalize() for word in x.replace('_', ' ').split())\n",
    ")\n",
    "\n",
    "# Save the DataFrame to a CSV file (optional)\n",
    "consolidated_df.to_excel('sec_universities.xlsx', index=False)\n",
    "consolidated_df.to_csv('sec_universities.csv', index=False)\n",
    "display(consolidated_df)\n",
    "\n",
    "# Calculate elapsed time\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Finished processing {len(consolidated_df)} universities in {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scraping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
